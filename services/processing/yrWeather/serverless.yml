# More documentation here: https://serverless.com/framework/docs/providers/aws/guide/serverless.yml/
service: yrWeatherProcessor

custom:
  #Edit these variables#########################################
  editable:
    timeout: 6 #seconds
    handlerFile: yr_processor
    databaseName: yr_weather
    description: Processing yr data from s3 to database
    accessLevel: level-1
    dataFolder: yr
  ##############################################################
  stage: ${opt:stage, self:provider.stage} #define stage here to include opt:stage, if stage option is passed in with sls deploy
  accessPath: data/${self:custom.editable.accessLevel}/${self:custom.editable.dataFolder}/
  pythonRequirements:
    dockerizePip: non-linux
    noDeploy: # Default in lambdas
      - boto3
      - botocore

provider:
  name: aws
  stage: dev
  region: eu-central-1
  stackName: ${self:custom.stage}-${self:service}
  deploymentBucket:
    name: dataplattform-v2-deploymentbucket # Name of s3 bucket
  runtime: python3.7
  memorySize: 1024 # MB
  timeout: ${self:custom.editable.timeout} # seconds
  tags: # Tags that will be added to each of the deployment resources.
    Project: Dataplattform # When used for sub-projects like eventBox change project tag to eventBox
    Layer: Processing # ingestion, processing, restApi, infrastructure
    #Ingestion: Pollers #webHooks, Pollers, iot
    #processing: linkedIn
    #restApi: linkedIn
    #infrastructure: s3
    # key: value
  stackTags: # Tags for cloud formation
    # key: value
  environment: # Service wide environment variables
    #ENVIRONMENT_VARIABLE_NAME: 123456789

package:
  individually: true
  exclude:
    - "./**" # Exclude everything and include what you need in the functions below

functions:
  processor:
    handler: ${self:custom.editable.handlerFile}.handler # fileName.functionNameInFile
    name: ${self:custom.stage}-${self:service} # Always begin name with stage
    description: ${self:custom.editable.description}
    role: !GetAtt S3Access.Arn
    package:
      include:
        - '*.py' # Files to include in deployment
    environment: # Environment variables, often stored in SSM
      #ENVIRONMENT_VARIABLE_NAME: ${ssm:ssm_key}
      STAGE: ${self:custom.stage}
      DATALAKE: !ImportValue ${self:custom.stage}-datalakeName
      ACCESS_PATH: ${self:custom.accessPath}
    tags: # Tag for this function. Every function are tagged with stage by default
    events:
      - s3: # Lambda fires when object is created in s3
          bucket: !ImportValue ${self:custom.stage}-datalakeName
          event: s3:ObjectCreated:*
          rules: # OBject must be in within this path GoogleCalendarEvents/**.json
            - prefix: ${self:custom.accessPath}
            - suffix: .json
          existing: true


resources: # The resources your functions use
  Resources:
    GlueDatabase:
      Type: AWS::Glue::Database
      Properties: 
        CatalogId: !Sub '#{AWS::AccountId}'
        DatabaseInput: 
          Name:  ${self:custom.stage}_${self:custom.editable.databaseName}_database
    GlueTable:
      Type: AWS::Glue::Table
      Properties:
        CatalogId: !Sub '#{AWS::AccountId}'
        DatabaseName: ${self:custom.stage}_${self:custom.editable.databaseName}_database
        TableInput: 
          Name: ${file(./schema.yml):tableName}
          Parameters:
            'classification': 'parquet'
            'parquet.compression': 'GZIP'
          PartitionKeys: ${file(./schema.yml):partitions}
          StorageDescriptor: 
            Columns: ${file(./schema.yml):columns}
            InputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
            Location: 
              Fn::Join:
                - "" 
                - - s3://
                  - !ImportValue ${self:custom.stage}-datalakeName
                  - /${self:custom.accessPath}database/${file(./schema.yml):tableName}/
            OutputFormat: org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
            SerdeInfo: 
              SerializationLibrary: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
              Parameters:
                'serialization.format': '1'
          TableType: EXTERNAL_TABLE
    S3Access:
      Type: AWS::IAM::Role
      Properties:
        RoleName: ${self:custom.stage}-${self:service}
        AssumeRolePolicyDocument:
          Version: "2012-10-17"
          Statement:
            - Effect: Allow
              Principal:
                Service:
                  - lambda.amazonaws.com
              Action: sts:AssumeRole
        ManagedPolicyArns:
          - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
          - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
        Policies: # Give lambda access to s3 data
          - PolicyName: ${self:custom.stage}-DatalakeRead-${self:service}
            PolicyDocument:
              Version: "2012-10-17"
              Statement:
                - Effect: Allow
                  Action:
                    - s3:GetObject
                  Resource:
                    - Fn::Join:
                      - "" 
                      - - !ImportValue ${self:custom.stage}-datalakeArn
                        - /${self:custom.accessPath}*
          - PolicyName: ${self:custom.stage}-ParameterStore-${self:service}
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - 'ssm:GetParameter*'
                  Resource: !Sub 'arn:aws:ssm:#{AWS::Region}:#{AWS::AccountId}:parameter/${self:custom.stage}/rds/postgres/*' # Only give access to what the lambda needs
  Outputs: # The outputs that your AWS CloudFormation Stack should produce. This allows references between services.

plugins:
  - serverless-python-requirements # Installs python requirements from requirements.txt file in module foldervl
  - serverless-pseudo-parameters
